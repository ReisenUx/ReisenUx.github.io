<!DOCTYPE html>
<html lang="en">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="">
  <title>courseraWED | 月战老兵月球基地</title>
  <meta name="author" content="ReisenU" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="" />
  
  <meta name="description" content="一、Introductionsizex -&gt; neuron -&gt; pricey - What does the simplest neural network do?It inputs the , computes the linear function, take max of zero【最小值为0】, and then outputs the estimated price.  T">
<meta property="og:type" content="article">
<meta property="og:title" content="courseraWED">
<meta property="og:url" content="http://example.com/2022/03/01/courseraWED/index.html">
<meta property="og:site_name" content="月战老兵月球基地">
<meta property="og:description" content="一、Introductionsizex -&gt; neuron -&gt; pricey - What does the simplest neural network do?It inputs the , computes the linear function, take max of zero【最小值为0】, and then outputs the estimated price.  T">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/null">
<meta property="article:published_time" content="2022-03-01T09:52:50.000Z">
<meta property="article:modified_time" content="2022-03-30T08:43:53.926Z">
<meta property="article:author" content="ReisenU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/null">
<meta name="twitter:site" content="@null">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" type="text/css" media="all">
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" type="text/css" media="all">
  
  
  <link rel="stylesheet" id="fontawe-css" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css" media="all">
  <link rel="stylesheet" id="nprogress-css" href="https://unpkg.com/nprogress@0.2.0/nprogress.css" type="text/css" media="all">
  
  
    <link rel="stylesheet" href="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.css">
  
  
    <link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-dark.min.css" type="text/css" media="all">
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="https://unpkg.com/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="https://unpkg.com/qrcode_js@1.0.0/qrcode.min.js"></script>
  
  
  <style>
    
      .kratos-cover.kratos-cover-2 {
        background-image: url('/images/banner.webp');
      }
    
    
      @media(min-width:768px) {
        body.custom-background {
          background-image: url('/images/bg.webp');
        }
      }
    
  </style>
  
<meta name="generator" content="Hexo 6.0.0"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
                                    
                                
                                    
                                        <li><a href="/archives/"><i class="fa fa-file"></i>档案馆</a></li>
                                    
                                
                                    
                                        <li><a href="/friends/"><i class="fa fa-paw"></i>好伙伴</a></li>
                                    
                                
                                    
                                        <li><a href="/"><i class="fa fa-link"></i>链接</a></li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">月战老兵月球基地</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>月战老兵月球基地</h2> <br />
                        <span>Lunar Capital</span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        

            <section class="col-md-8">

        

            <article>
    <div class="kratos-hentry kratos-post-inner clearfix">
        <header class="kratos-entry-header">
            
                <h1 class="kratos-entry-title text-center">courseraWED</h1>
            
            
            <ul class="kratos-post-meta text-center">
                <li><i class="fa fa-calendar"></i> 2022-03-01</li>
                <li><i class="fa fa-user"></i> 作者 ReisenU</li>
                <li>
                    <i class="fa fa-edit"></i> 
                    
                    
                        ~6.30K
                    
                    字
                </li>
                
            </ul>
        </header>
        <div class="kratos-post-content">
            <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                本文最后编辑于 <time datetime="1648629833926"></time> 前，其中的内容可能需要更新。
            </div>
            
                <div class="kratos-post-inner-toc">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Introduction"><span class="toc-number">1.</span> <span class="toc-text">一、Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#What-does-the-simplest-neural-network-do"><span class="toc-number">1.1.</span> <span class="toc-text">- What does the simplest neural network do?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU-function-%E2%80%94%E2%80%94-A-kind-of-activation-function"><span class="toc-number">1.2.</span> <span class="toc-text">- ReLU function —— A kind of activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-learning"><span class="toc-number">1.3.</span> <span class="toc-text">- Supervised learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unstructured-x2F-Structured-Data"><span class="toc-number">1.4.</span> <span class="toc-text">- Unstructured&#x2F;Structured Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scale-drives-deep-learning-progress"><span class="toc-number">1.5.</span> <span class="toc-text">- Scale drives deep learning progress</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Basics-of-Neural-Network-Programming"><span class="toc-number">2.</span> <span class="toc-text">二、Basics of Neural Network Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Binary-Classification"><span class="toc-number">2.1.</span> <span class="toc-text">- Binary Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">2.2.</span> <span class="toc-text">- Logistic Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-Lost-Function"><span class="toc-number">2.3.</span> <span class="toc-text">- Logistic Regression Lost Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">2.4.</span> <span class="toc-text">- Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Computation-Graph"><span class="toc-number">2.5.</span> <span class="toc-text">- Computation Graph</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression"><span class="toc-number">2.6.</span> <span class="toc-text">- logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vectorization"><span class="toc-number">2.7.</span> <span class="toc-text">- vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#More-examples-of-vectorization"><span class="toc-number">2.8.</span> <span class="toc-text">- More examples of vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regression-of-logistic"><span class="toc-number">2.9.</span> <span class="toc-text">- Regression of logistic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorizing-Logistic-Regression"><span class="toc-number">2.10.</span> <span class="toc-text">- Vectorizing Logistic Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Broadcasting-in-Python"><span class="toc-number">2.11.</span> <span class="toc-text">- Broadcasting in Python</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tips-in-Python"><span class="toc-number">2.12.</span> <span class="toc-text">- Tips in Python</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-on-m-Examples"><span class="toc-number">2.13.</span> <span class="toc-text">- Cost on m Examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-network-represents"><span class="toc-number">2.14.</span> <span class="toc-text">- Neural network represents</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorizing-across-multiple-examples"><span class="toc-number">2.15.</span> <span class="toc-text">- Vectorizing across multiple examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Activation-functions"><span class="toc-number">2.16.</span> <span class="toc-text">- Activation functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-using-the-activation-function"><span class="toc-number">2.17.</span> <span class="toc-text">- Why using the activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sigmoid-activation-function"><span class="toc-number">2.18.</span> <span class="toc-text">- Sigmoid activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tanh-activation-function"><span class="toc-number">2.19.</span> <span class="toc-text">- Tanh activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU-and-Leaky-ReLU-activation-function"><span class="toc-number">2.20.</span> <span class="toc-text">- ReLU and Leaky ReLU activation function</span></a></li></ol></li></ol>
                </div>
            
            <hr />
            <h1 id="一、Introduction"><a href="#一、Introduction" class="headerlink" title="一、Introduction"></a>一、Introduction</h1><p>size<code>x</code> -&gt; neuron -&gt; price<code>y</code></p>
<h2 id="What-does-the-simplest-neural-network-do"><a href="#What-does-the-simplest-neural-network-do" class="headerlink" title="- What does the simplest neural network do?"></a>- What does the simplest neural network do?</h2><p>It inputs the , computes the linear function, take max of zero【最小值为0】, and then outputs the estimated price.</p>
<img src="/2022/03/01/courseraWED/SimpleNeuralNetwork.png" class="">
<p>This middle layer densely connected to the input layer, the network itself decides what the middle neuron refers to.<br>What you need to do is trying to take an input a X and map it some output Y.</p>
<h2 id="ReLU-function-——-A-kind-of-activation-function"><a href="#ReLU-function-——-A-kind-of-activation-function" class="headerlink" title="- ReLU function —— A kind of activation function"></a>- ReLU function —— A kind of activation function</h2><p>Rectified Linear Unit 【线性修正单元】<br>The function is a straight line which taking a max of zero.<br>Rectify just means taking a max of zero.</p>
<img src="/2022/03/01/courseraWED/ReLU.png" class="">

<h2 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="- Supervised learning"></a>- Supervised learning</h2><ol>
<li>Real Estate&#x2F;Online Advertising: Standard NN</li>
<li>Image application: Convolutional Neural Network【abbreviated: CNN】</li>
<li>Sequence Data(One-dimensional)【Audio&#x2F;language】: Recurrent Neural Network【abbreviated: RNN】</li>
<li>Complex Application【autonomous driving】: Custom&#x2F;Hybrid【混合】<img src="/2022/03/01/courseraWED/DifferentNN.png" class=""></li>
</ol>
<h2 id="Unstructured-x2F-Structured-Data"><a href="#Unstructured-x2F-Structured-Data" class="headerlink" title="- Unstructured&#x2F;Structured Data"></a>- Unstructured&#x2F;Structured Data</h2><ol>
<li>Structured data: Each of the features have the well defined meaning.【Just like database】</li>
<li>Unstructured data: Like audio&#x2F;images&#x2F;text<img src="/2022/03/01/courseraWED/StructuredOrUn.png" class=""></li>
</ol>
<h2 id="Scale-drives-deep-learning-progress"><a href="#Scale-drives-deep-learning-progress" class="headerlink" title="- Scale drives deep learning progress"></a>- Scale drives deep learning progress</h2><ol>
<li>Size of the NN. A NN with a lot of hidden units&#x2F;parameters&#x2F;connections</li>
<li>Large amount of the 【labeled】 data.</li>
</ol>
<p>  m, denote the size of training sets.</p>
<blockquote>
<p>NOTICE: If using smaller training sets, the relative ordering of the algorithms is not well defined so.</p>
</blockquote>
<blockquote>
<p>Switching the sigmoid function to the ReLU function has made the algorithm called gradient descent work much faster.<br>The impact of the algorithmic innovation was it really help compution.</p>
</blockquote>
<h1 id="二、Basics-of-Neural-Network-Programming"><a href="#二、Basics-of-Neural-Network-Programming" class="headerlink" title="二、Basics of Neural Network Programming"></a>二、Basics of Neural Network Programming</h1><h2 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="- Binary Classification"></a>- Binary Classification</h2><blockquote>
<p>Notations<br>(x, y) —— x:R<sup>n<sub>x</sub></sup> —  y:{0, 1}<br>m【Training example】 —— {(x<sup>(1)</sup>, y<sup>(1)</sup>) …… (x<sup>(m)</sup>, y<sup>(m)</sup>)}<br>m<sub>train</sub> &#x3D; m —— m<sub>test</sub> &#x3D; #test_examples<br>X —— R<sup>n<sub>x<em>m</sub></sup>（matrix）—— X.shape &#x3D; (n<sub>x</sub> * m)<br>Y —— R<sup>1</em>m</sup> —— Y.shape &#x3D; (1, m)</p>
</blockquote>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="- Logistic Regression"></a>- Logistic Regression</h2><blockquote>
<p>Given <code>x</code>, want <code>y_hat = P(y=1|x)</code><br>x &#x3D; R<sup>n<sub>x</sub></sup><br>Parameters: w &#x3D; R<sup>n<sub>x</sub></sup>, b &#x3D; R<br>Output【Just a try】: <code>y_hat</code> &#x3D; Sigmoid(w<sup>T</sup> * x + b)【The result may not in the range of 0 to 1】</p>
</blockquote>
<img src="/2022/03/01/courseraWED/Logistic.png" class="">
<img src="/2022/03/01/courseraWED/LogisticMath.png" class="">

<h2 id="Logistic-Regression-Lost-Function"><a href="#Logistic-Regression-Lost-Function" class="headerlink" title="- Logistic Regression Lost Function"></a>- Logistic Regression Lost Function</h2><blockquote>
<p>Notice the Optimization Problem that is convex【Prevent Local Optimum And Try to converge(close) to the Global Optimum】</p>
</blockquote>
<img src="/2022/03/01/courseraWED/LossFunction.png" class="" title="CrossEntropy">

<p>Loss Function: A single training example. —— Measure how well the network is doing.<br>Cost Function: Entire training example. —— Cost of the parameters.【minimize it】</p>
<img src="/2022/03/01/courseraWED/CostFunction.png" class="" title="CostFunction">

<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="- Gradient Descent"></a>- Gradient Descent</h2><p>Want to find (w, b) that minimize J(w, b)【The cost function】</p>
<img src="/2022/03/01/courseraWED/LossFunctionGraph.png" class="" title="Convex-Function">
<p>The formula of Gradient Descent is as follow:</p>
<img src="/2022/03/01/courseraWED/GradientDescent.png" class="" title="formula">

<h2 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="- Computation Graph"></a>- Computation Graph</h2><p>The computation graph organizes a computation with blue arrow left to right computation.</p>
<img src="/2022/03/01/courseraWED/ComputationGraph.png" class="" title="ComputationGraph">
<p>We should handle the red arrow right to left computation of the derivatives.</p>
<p>Cross Entropy: -(ylog(a) + (1-y)log(1-a)) where y is the ground_truth and a is the output of the network</p>
<h2 id="logistic-regression"><a href="#logistic-regression" class="headerlink" title="- logistic regression"></a>- logistic regression</h2><p>The goal is to modify the parameter <code>w,b</code> and to reduce the loss <code>L(a, y)</code><br>Using Computation Graph, and from output to the parameters, calculate the derivative of the loss function【Cross Entropy】</p>
<p>logistic regression on m examples<br>Notice the right part means the gradient descent, w1 :&#x3D; w1 - α * dw1, where α is the learning rate and dw1 is the derivative</p>
<img src="/2022/03/01/courseraWED/mExamplesOfLogisticRegression.png" class="">

<p>In order to getting rid of <code>for</code> loops, it’s perferable to use vectorization</p>
<h2 id="vectorization"><a href="#vectorization" class="headerlink" title="- vectorization"></a>- vectorization</h2><p>For example, in Python with numpy<br>np.dot(w, b)【dot即点乘】</p>
<p>Using vectorization can significantly speed up the code rather than using for loop.</p>
<blockquote>
<p>Avoid using explicit for loops</p>
</blockquote>
<h2 id="More-examples-of-vectorization"><a href="#More-examples-of-vectorization" class="headerlink" title="- More examples of vectorization"></a>- More examples of vectorization</h2><img src="/2022/03/01/courseraWED/UsingVectorization1.png" class="" title="向量化例子1">

<img src="/2022/03/01/courseraWED/UsingVectorization2.png" class="" title="向量化例子2">

<h2 id="Regression-of-logistic"><a href="#Regression-of-logistic" class="headerlink" title="- Regression of logistic"></a>- Regression of logistic</h2><blockquote>
<p>recap：the forward propagation step is to compute the predictions.<br>       the backward propagation step is using the result of prediction to modify the parameters.</p>
</blockquote>
<img src="/2022/03/01/courseraWED/LogisticForwardPropagation.png" class="" title="Logistic前向传播">
<p>A vectorization implementation of the forward propagation for all M training examples at the same time.</p>
<p>Actually, we can use vectorization to calculate the backward propagation to compute the gradients.</p>
<h2 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="- Vectorizing Logistic Regression"></a>- Vectorizing Logistic Regression</h2><img src="/2022/03/01/courseraWED/VectorizingLogisticRegression.png" class="">

<img src="/2022/03/01/courseraWED/VectorizingLogisticRegression2.png" class="">



<h2 id="Broadcasting-in-Python"><a href="#Broadcasting-in-Python" class="headerlink" title="- Broadcasting in Python"></a>- Broadcasting in Python</h2><p>cal &#x3D; A.sum(axis &#x3D; 0) 【0 means sum vertically, where horizontal axis is 1】<br>percentage &#x3D; 100 * A &#x2F; cal.reshape(1, 4)【(3, 4) &#x2F; (1, 4)】</p>
<img src="/2022/03/01/courseraWED/BroadcastingExamples.png" class="">
<img src="/2022/03/01/courseraWED/BroadcastingGeneralPrinciple.png" class="">

<h2 id="Tips-in-Python"><a href="#Tips-in-Python" class="headerlink" title="- Tips in Python"></a>- Tips in Python</h2><ul>
<li><p>rank 1 array  —— Don’t use<br>a &#x3D; np.random.randn(5)<br>a.shape &#x3D; (5, )</p>
<blockquote>
<p>Using reshape can transfer the rank_1 array to the vector</p>
</blockquote>
</li>
<li><p>column&#x2F;row vector<br>a &#x3D; np.random.randn(5, 1) —— a.shape &#x3D; (5, 1) —— column vector<br>a &#x3D; np.random.randn(1, 5) —— a.shape &#x3D; (1, 5) —— row vector</p>
</li>
<li><p>assertion<br>assert(a.shape &#x3D;&#x3D; (5, 1))</p>
</li>
</ul>
<h2 id="Cost-on-m-Examples"><a href="#Cost-on-m-Examples" class="headerlink" title="- Cost on m Examples"></a>- Cost on m Examples</h2><p>Using the principle of maximum likelihood estimation</p>
<p>The log function is strict monotonically increasing function<br>So maximum log(p) give the similar result that optimizing p.</p>
<img src="/2022/03/01/courseraWED/LogisticFunctionLog.png" class="">

<img src="/2022/03/01/courseraWED/LogisticLikelihood.png" class="">

<h2 id="Neural-network-represents"><a href="#Neural-network-represents" class="headerlink" title="- Neural network represents"></a>- Neural network represents</h2><img src="/2022/03/01/courseraWED/NeuralNetworkRepresent.png" class="">

<h2 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="- Vectorizing across multiple examples"></a>- Vectorizing across multiple examples</h2><p>The horizontal, the matrix A goes over different training examples<br>Vertically, the different indices in the matrix.【Different input features or Different hidden units】</p>


<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="- Activation functions"></a>- Activation functions</h2><p>tanh function is always do better than sigmoid function</p>
<p>The result of tanh function is in the range of [-1, 1]</p>
<p>Though the output is in the range of [0, 1], so sigmoid function is better for the output layer.</p>
<p>The general setback of tanh&#x2F;sigmoid function is that when <code>z</code> is either very large or very small, the gradient of the derivative or the slope of this function becomes small.<br>In this way, the result will slow down the gradient descent.</p>
<p>A popular function is ReLU, Rectified Linear Unit. <code>ReLU = max(0, z)</code><br>The derivative of ReLU is 1 so long as z is positive, the slope is 0 when z is negative</p>
<p>The odds you get exactly z equals to 0 is very small.【The derivative in the point of zero is not defined】</p>
<p>Also we can try Leaky ReLU —— <code>Leaky ReLU = max(0.01z, z)</code></p>
<h2 id="Why-using-the-activation-function"><a href="#Why-using-the-activation-function" class="headerlink" title="- Why using the activation function"></a>- Why using the activation function</h2><p>If using linear activation functions or identity activation functions<br>then the neural network is just outputting a <b>linear function</b> of the input</p>
<p>If we do not use the activation function, what we do is just computing a linear activation function.<br>In that case…Why not delete all the hidden layers?</p>
<p>The linear hidden units is so long as not use them, they are less or more useless.</p>
<p>The only place we can use linear function is the output layer【house pricing prediction】【Why not ReLU?】</p>
<h2 id="Sigmoid-activation-function"><a href="#Sigmoid-activation-function" class="headerlink" title="- Sigmoid activation function"></a>- Sigmoid activation function</h2><img src="/2022/03/01/courseraWED/ProveOfSigmoid.png" class="">

<h2 id="Tanh-activation-function"><a href="#Tanh-activation-function" class="headerlink" title="- Tanh activation function"></a>- Tanh activation function</h2><img src="/2022/03/01/courseraWED/ProveOfTanh.png" class="">

<h2 id="ReLU-and-Leaky-ReLU-activation-function"><a href="#ReLU-and-Leaky-ReLU-activation-function" class="headerlink" title="- ReLU and Leaky ReLU activation function"></a>- ReLU and Leaky ReLU activation function</h2><img src="/2022/03/01/courseraWED/ProveOfReLU.png" class="">

<p>We can define the derivative of point 0 by ourselves.</p>

        </div>
        
            <div class="kratos-copyright text-center clearfix">
                <h5>本作品采用 <a rel="license nofollow" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 国际许可协议</a> 进行许可</h5>
            </div>
        
        <footer class="kratos-entry-footer clearfix">
            
                <div class="post-like-donate text-center clearfix" id="post-like-donate">
                
                
                    <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> 分享</a>
                    <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "http://example.com/2022/03/01/courseraWED/",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "http://example.com/2022/03/01/courseraWED/";
            const title         = "「courseraWED」";
            const excerpt       = `一、Introductionsizex -&gt; neuron -&gt; pricey
- What does the simplest neural network do?It inputs the , computes the...`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                
                </div>
            
            <div class="footer-tag clearfix">
                <div class="pull-left">
                <i class="fa fa-tags"></i>
                    
                </div>
                <div class="pull-date">
                <span>最后编辑：2022-03-30</span>
                </div>
            </div>
        </footer>
    </div>
    
        <nav class="navigation post-navigation clearfix" role="navigation">
            
            <div class="nav-previous clearfix">
                <a title=" 操作系统" href="/2022/02/28/OperatingSystem/">&lt; 上一篇</a>
            </div>
            
            
            <div class="nav-next clearfix">
                <a title=" cpp" href="/2022/03/02/cpp/">下一篇 &gt;</a>
            </div>
            
        </nav>
    
    
</article>

        

            </section>

        

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/Ru.webp" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center">人间归离复归离，借一浮生逃浮生</p>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix">
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        文章目录
        <span class="toc-progress-bar"></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Introduction"><span class="toc-text">一、Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#What-does-the-simplest-neural-network-do"><span class="toc-text">- What does the simplest neural network do?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU-function-%E2%80%94%E2%80%94-A-kind-of-activation-function"><span class="toc-text">- ReLU function —— A kind of activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-learning"><span class="toc-text">- Supervised learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unstructured-x2F-Structured-Data"><span class="toc-text">- Unstructured&#x2F;Structured Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scale-drives-deep-learning-progress"><span class="toc-text">- Scale drives deep learning progress</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Basics-of-Neural-Network-Programming"><span class="toc-text">二、Basics of Neural Network Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Binary-Classification"><span class="toc-text">- Binary Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression"><span class="toc-text">- Logistic Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression-Lost-Function"><span class="toc-text">- Logistic Regression Lost Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-text">- Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Computation-Graph"><span class="toc-text">- Computation Graph</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression"><span class="toc-text">- logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vectorization"><span class="toc-text">- vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#More-examples-of-vectorization"><span class="toc-text">- More examples of vectorization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regression-of-logistic"><span class="toc-text">- Regression of logistic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorizing-Logistic-Regression"><span class="toc-text">- Vectorizing Logistic Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Broadcasting-in-Python"><span class="toc-text">- Broadcasting in Python</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tips-in-Python"><span class="toc-text">- Tips in Python</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cost-on-m-Examples"><span class="toc-text">- Cost on m Examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-network-represents"><span class="toc-text">- Neural network represents</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Vectorizing-across-multiple-examples"><span class="toc-text">- Vectorizing across multiple examples</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Activation-functions"><span class="toc-text">- Activation functions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-using-the-activation-function"><span class="toc-text">- Why using the activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sigmoid-activation-function"><span class="toc-text">- Sigmoid activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tanh-activation-function"><span class="toc-text">- Tanh activation function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReLU-and-Leaky-ReLU-activation-function"><span class="toc-text">- ReLU and Leaky ReLU activation function</span></a></li></ol></li></ol>
    </div>
</aside>
                
                

            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>标签聚合</h4>
      <div class="tag-clouds">
        <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 0.6em;">动态规划</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 0.8em;">数据库</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.6em;">算法</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 0.6em;">计算机网络</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/2022/03/30/Git/"><i class="fa  fa-book"></i> Git</a>
            
          
        
          
          
            <a class="list-group-item" href="/2022/03/28/cppRest/"><i class="fa  fa-book"></i> cppRest</a>
            
          
        
          
          
            <a class="list-group-item" href="/2022/03/27/cpp11/"><i class="fa  fa-book"></i> cpp11</a>
            
          
        
          
          
            <a class="list-group-item" href="/2022/03/26/cppMemory/"><i class="fa  fa-book"></i> cppMemory</a>
            
          
        
          
          
            <a class="list-group-item" href="/2022/03/24/STL/"><i class="fa  fa-book"></i> STL</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        
                        
                        <li><a target="_blank" rel="nofollow" href="https://t.me/ReisenU"><i class="fa fa-telegram"></i></a></li>
                        
                        
                        
                        <li><a target="_blank" rel="me" href="https://nya.one/@ReisenU"><i class="fa fa fa-share-alt-square"></i></a></li>
                        <li><a target="_blank" rel="nofollow" href="https://github.com/ReisenUx"><i class="fa fa-github"></i></a></li>
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2022 月战老兵月球基地 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by ReisenU.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.io" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="https://unpkg.com/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="https://unpkg.com/nprogress@0.2.0/nprogress.js"></script>
<script>
    if (!window.kr) {
        window.kr = {};
    }
    window.kr.notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));
    window.kr.siteRoot = "/";
</script>




    <script defer src="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="https://unpkg.com/meting@2/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="7292006123"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="https://unpkg.com/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>


    <script defer src="/js/kr-dark.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>